{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfGFWhLgzmmN",
        "outputId": "b9e60e8f-90b5-4701-aaaa-61f3b281a825"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define dataset path\n",
        "dataset_zip_path = '/content/dataset.zip'  # Adjust the file name if needed\n",
        "dataset_extract_path = '/content/dataset'\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l5dZ9Xy0Ise",
        "outputId": "868f3ec3-57af-4248-ada5-ecbf851113f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def extract_frames_at_intervals(video_path, output_dir, interval=10, resize_shape=(224, 224)):\n",
        "    \"\"\"\n",
        "    Extract frames from a video at a fixed interval and save them in a directory.\n",
        "\n",
        "    :param video_path: Path to the video file.\n",
        "    :param output_dir: Directory to save the frames.\n",
        "    :param interval: Number of frames to skip between each extraction.\n",
        "    :param resize_shape: Tuple (width, height) to resize the frames.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        video = cv2.VideoCapture(video_path)\n",
        "        if not video.isOpened():\n",
        "            print(f\"Error opening video file: {video_path}\")\n",
        "            return\n",
        "\n",
        "        frame_count = 0\n",
        "        saved_frame_count = 0\n",
        "\n",
        "        while True:\n",
        "            ret, frame = video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Save frame at the specified interval\n",
        "            if frame_count % interval == 0:\n",
        "                frame = cv2.resize(frame, resize_shape)\n",
        "                frame_path = os.path.join(output_dir, f\"frame_{saved_frame_count:04d}.jpg\")\n",
        "                cv2.imwrite(frame_path, frame)\n",
        "                saved_frame_count += 1\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        video.release()\n",
        "        print(f\"Extracted {saved_frame_count} frames from {video_path} at interval {interval} and saved to {output_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting frames: {e}\")\n",
        "\n",
        "# Use the correct video path\n",
        "video_path = '/content/dataset/dataset/003_anonymized.mp4'\n",
        "output_dir = '/content/frames'  # Directory to save extracted frames\n",
        "\n",
        "# Extract frames at intervals of 10\n",
        "extract_frames_at_intervals(video_path, output_dir, interval=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lq2Jm_m67z4",
        "outputId": "5c4b5f22-c97c-48a8-f131-7276cfaf1755"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 5911 frames from /content/dataset/dataset/003_anonymized.mp4 at interval 10 and saved to /content/frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "def compare_and_classify_frames_with_motion_detection(input_dir, output_dir, motion_threshold=500):\n",
        "    \"\"\"\n",
        "    Use motion detection to classify frames as robot_off or robot_working.\n",
        "\n",
        "    :param input_dir: Directory containing the extracted frames.\n",
        "    :param output_dir: Base directory for saving classified frames.\n",
        "    :param motion_threshold: Minimum number of changed pixels to classify as working.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create directories for robot_off and robot_working\n",
        "        off_dir = os.path.join(output_dir, \"robot_off\")\n",
        "        working_dir = os.path.join(output_dir, \"robot_working\")\n",
        "        os.makedirs(off_dir, exist_ok=True)\n",
        "        os.makedirs(working_dir, exist_ok=True)\n",
        "\n",
        "        # Sort frame files to ensure proper sequence\n",
        "        frame_files = sorted(os.listdir(input_dir))\n",
        "        frame_paths = [os.path.join(input_dir, file) for file in frame_files]\n",
        "\n",
        "        for i in range(1, len(frame_paths)):\n",
        "            prev_frame = cv2.imread(frame_paths[i - 1], cv2.IMREAD_GRAYSCALE)\n",
        "            curr_frame = cv2.imread(frame_paths[i], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Skip if frames cannot be read\n",
        "            if prev_frame is None or curr_frame is None:\n",
        "                print(f\"Skipping frame {frame_files[i]} due to read error.\")\n",
        "                continue\n",
        "\n",
        "            # Compute absolute difference\n",
        "            diff = cv2.absdiff(curr_frame, prev_frame)\n",
        "\n",
        "            # Apply a binary threshold\n",
        "            _, diff_thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "            # Count non-zero pixels\n",
        "            motion_pixels = cv2.countNonZero(diff_thresh)\n",
        "\n",
        "            # Classify based on motion detection\n",
        "            if motion_pixels > motion_threshold:\n",
        "                # Robot is working\n",
        "                shutil.copy(frame_paths[i], os.path.join(working_dir, frame_files[i]))\n",
        "            else:\n",
        "                # Robot is off\n",
        "                shutil.copy(frame_paths[i], os.path.join(off_dir, frame_files[i]))\n",
        "\n",
        "        print(f\"Frames classified and saved to {output_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying frames: {e}\")\n",
        "\n",
        "# Example usage\n",
        "input_frames_dir = '/content/frames'  # Directory where reduced frames are stored\n",
        "classified_output_dir = '/content/classified_frames'  # Directory to save classified frames\n",
        "\n",
        "# Classify the frames\n",
        "compare_and_classify_frames_with_motion_detection(input_frames_dir, classified_output_dir, motion_threshold=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AooCh62G0vzn",
        "outputId": "b134c267-649a-4c93-9110-7fa68bb0c8c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames classified and saved to /content/classified_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in [\"robot_off\", \"robot_working\"]:\n",
        "    folder_path = os.path.join(classified_output_dir, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        print(f\"Found {folder}: {len(os.listdir(folder_path))} frames\")\n",
        "    else:\n",
        "        print(f\"{folder} not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QV0DKJn1KjO",
        "outputId": "3c73481e-aebf-487c-cadd-08bf175b6bd3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found robot_off: 3985 frames\n",
            "Found robot_working: 1925 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class FrameDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Data generator for loading frames in batches during training.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dir, label_map, batch_size=32, resize_shape=(224, 224), shuffle=True):\n",
        "        self.base_dir = base_dir\n",
        "        self.label_map = label_map\n",
        "        self.batch_size = batch_size\n",
        "        self.resize_shape = resize_shape\n",
        "        self.shuffle = shuffle\n",
        "        self.file_paths, self.labels = self._load_file_paths()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _load_file_paths(self):\n",
        "        \"\"\"\n",
        "        Load file paths and corresponding labels from the base directory.\n",
        "        \"\"\"\n",
        "        file_paths = []\n",
        "        labels = []\n",
        "        for folder, label in self.label_map.items():\n",
        "            folder_path = os.path.join(self.base_dir, folder)\n",
        "            if not os.path.exists(folder_path):\n",
        "                print(f\"Warning: Folder {folder} does not exist.\")\n",
        "                continue\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.endswith(\".jpg\"):\n",
        "                    file_paths.append(os.path.join(folder_path, file))\n",
        "                    labels.append(label)\n",
        "        return file_paths, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of batches per epoch.\n",
        "        \"\"\"\n",
        "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one batch of data.\n",
        "        \"\"\"\n",
        "        start = index * self.batch_size\n",
        "        end = (index + 1) * self.batch_size\n",
        "        batch_paths = self.file_paths[start:end]\n",
        "        batch_labels = self.labels[start:end]\n",
        "\n",
        "        images = []\n",
        "        for file_path in batch_paths:\n",
        "            img = cv2.imread(file_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, self.resize_shape) / 255.0  # Normalize\n",
        "            images.append(img)\n",
        "        return np.array(images), np.array(batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Shuffle data at the end of each epoch.\n",
        "        \"\"\"\n",
        "        if self.shuffle:\n",
        "            combined = list(zip(self.file_paths, self.labels))\n",
        "            np.random.shuffle(combined)\n",
        "            self.file_paths, self.labels = zip(*combined)"
      ],
      "metadata": {
        "id": "ticaxO6292Df"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label map\n",
        "label_map = {\"robot_off\": 0, \"robot_working\": 1}\n",
        "\n",
        "# Define paths\n",
        "classified_data_dir = '/content/classified_frames'\n",
        "\n",
        "# Create generators\n",
        "batch_size = 32\n",
        "train_generator = FrameDataGenerator(base_dir=classified_data_dir, label_map=label_map, batch_size=batch_size, resize_shape=(224, 224), shuffle=True)\n",
        "test_generator = FrameDataGenerator(base_dir=classified_data_dir, label_map=label_map, batch_size=batch_size, resize_shape=(224, 224), shuffle=False)"
      ],
      "metadata": {
        "id": "tP_AaHA8-e1u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model (same as before)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    \"\"\"\n",
        "    Create a CNN model for binary classification.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "input_shape = (224, 224, 3)\n",
        "model = create_cnn_model(input_shape)\n",
        "\n",
        "# Train the model using the generator\n",
        "model.fit(train_generator, validation_data=test_generator, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lq83CDI-f1F",
        "outputId": "8d7df545-afa4-4466-c542-3cde416d468e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 4s/step - accuracy: 0.7416 - loss: 1.0353 - val_accuracy: 0.8794 - val_loss: 0.3050\n",
            "Epoch 2/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m785s\u001b[0m 4s/step - accuracy: 0.8879 - loss: 0.3012 - val_accuracy: 0.9115 - val_loss: 0.2815\n",
            "Epoch 3/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 4s/step - accuracy: 0.8882 - loss: 0.2644 - val_accuracy: 0.9140 - val_loss: 0.1964\n",
            "Epoch 4/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m822s\u001b[0m 4s/step - accuracy: 0.9089 - loss: 0.2223 - val_accuracy: 0.9267 - val_loss: 0.1706\n",
            "Epoch 5/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 4s/step - accuracy: 0.9224 - loss: 0.1929 - val_accuracy: 0.9315 - val_loss: 0.1600\n",
            "Epoch 6/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 4s/step - accuracy: 0.9194 - loss: 0.1877 - val_accuracy: 0.9360 - val_loss: 0.1385\n",
            "Epoch 7/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 4s/step - accuracy: 0.9301 - loss: 0.1643 - val_accuracy: 0.9421 - val_loss: 0.1371\n",
            "Epoch 8/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 4s/step - accuracy: 0.9397 - loss: 0.1526 - val_accuracy: 0.9406 - val_loss: 0.1390\n",
            "Epoch 9/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 4s/step - accuracy: 0.9322 - loss: 0.1570 - val_accuracy: 0.9487 - val_loss: 0.1166\n",
            "Epoch 10/10\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 4s/step - accuracy: 0.9377 - loss: 0.1498 - val_accuracy: 0.9543 - val_loss: 0.1143\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e0cbbc4bdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erNJlw-7Vctg",
        "outputId": "4b7f9948-8462-4170-d01d-18e49db10f1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 956ms/step - accuracy: 0.9642 - loss: 0.0840\n",
            "Test Loss: 0.11431056261062622\n",
            "Test Accuracy: 0.9543147087097168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def detect_start_stop_timings(input_dir, threshold=500, fps=30):\n",
        "    \"\"\"\n",
        "    Detect start and stop timings of a cobot based on motion in frames.\n",
        "\n",
        "    :param input_dir: Directory containing the extracted frames.\n",
        "    :param threshold: Minimum number of changed pixels to detect motion.\n",
        "    :param fps: Frames per second of the video, used to calculate time.\n",
        "    :return: DataFrame with start and stop timings.\n",
        "    \"\"\"\n",
        "    # Sort frame files to ensure proper sequence\n",
        "    frame_files = sorted(os.listdir(input_dir))\n",
        "    frame_paths = [os.path.join(input_dir, file) for file in frame_files]\n",
        "\n",
        "    start_time = None\n",
        "    stop_time = None\n",
        "    timings = []\n",
        "\n",
        "    for i in range(1, len(frame_paths)):\n",
        "        prev_frame = cv2.imread(frame_paths[i - 1], cv2.IMREAD_GRAYSCALE)\n",
        "        curr_frame = cv2.imread(frame_paths[i], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Skip if frames cannot be read\n",
        "        if prev_frame is None or curr_frame is None:\n",
        "            print(f\"Skipping frame {frame_files[i]} due to read error.\")\n",
        "            continue\n",
        "\n",
        "        # Compute absolute difference\n",
        "        diff = cv2.absdiff(curr_frame, prev_frame)\n",
        "\n",
        "        # Apply a binary threshold\n",
        "        _, diff_thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Count non-zero pixels\n",
        "        motion_pixels = cv2.countNonZero(diff_thresh)\n",
        "\n",
        "        # Detect start and stop times\n",
        "        if motion_pixels > threshold:\n",
        "            if start_time is None:\n",
        "                # Start detected\n",
        "                start_time = i / fps  # Convert frame number to time\n",
        "        else:\n",
        "            if start_time is not None:\n",
        "                # Stop detected\n",
        "                stop_time = (i - 1) / fps  # Convert frame number to time\n",
        "                timings.append({\"Start Time (s)\": start_time, \"Stop Time (s)\": stop_time})\n",
        "                start_time = None  # Reset start time for the next motion segment\n",
        "\n",
        "    # Handle case where video ends during motion\n",
        "    if start_time is not None:\n",
        "        timings.append({\"Start Time (s)\": start_time, \"Stop Time (s)\": len(frame_paths) / fps})\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    timing_df = pd.DataFrame(timings)\n",
        "    return timing_df\n",
        "\n",
        "# Example usage\n",
        "input_frames_dir = '/content/frames'\n",
        "fps = 30  # Frames per second of the video\n",
        "timing_df = detect_start_stop_timings(input_frames_dir, threshold=500, fps=fps)\n",
        "\n",
        "# Display the table\n",
        "print(\"Cobot Start and Stop Timings:\")\n",
        "print(timing_df)\n",
        "\n",
        "# Optionally save the table to a CSV file\n",
        "timing_df.to_csv('/content/cobot_timing.csv', index=False)\n",
        "print(\"Timings saved to cobot_timing.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx3zwsTK1o5H",
        "outputId": "e72cfd9a-dc27-441d-aaed-765053a150c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cobot Start and Stop Timings:\n",
            "     Start Time (s)  Stop Time (s)\n",
            "0          0.100000       0.100000\n",
            "1          0.166667       0.233333\n",
            "2          0.333333       0.566667\n",
            "3          0.700000       1.166667\n",
            "4          1.333333       1.400000\n",
            "..              ...            ...\n",
            "226      185.033333     185.233333\n",
            "227      189.366667     189.633333\n",
            "228      190.700000     190.966667\n",
            "229      193.766667     193.866667\n",
            "230      193.933333     195.200000\n",
            "\n",
            "[231 rows x 2 columns]\n",
            "Timings saved to cobot_timing.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/robot_classification_model.h5')\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03GLCgLw2VpO",
        "outputId": "7c9ce1a4-fa1e-4ecd-890a-989a29b0cd59"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('/content/robot_classification_model.h5')\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMwgyUvu2ZZQ",
        "outputId": "7fadbb79-051a-47b0-f765-45b4c75fed3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    }
  ]
}